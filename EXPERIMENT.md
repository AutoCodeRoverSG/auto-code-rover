# Running experiments on SWE-bench

## Setup

_Recommend environment: Ubuntu 20.04. (Other environments have not been extensively tests.)_

Currently AutoCodeRover runs inference on SWE-bench tasks in a local environment, assuming
the SWE-bench task's source code and conda environment has been set up locally on the same machine.
For details on how to set up SWE-bench task instances for running with ACR, see the "SWE-bench mode" section in [README.md](README.md).

After setting up the tasks and put them into a file `tasks.txt`, prepare a conf file for the experiment.
An example conf file can be found in `conf/example.conf`. Here are what each field in the conf file means:

- id: determines the name of the experiment output folder.
- date: a date string for book-keeping.
- experiment_dir: where output will be stored.
- setup_result_dir: must point to the directory where SWE-bench setup writes its result.
- swe_bench_dir: path to the SWE-bench directory used in the previous setup steps.
- docker_swe_bench_dir: path to the SWE-bench-docker directory. This directory is already included at root-level of this code package.

- model: the model to be used by auto-code-rover.
- temperature: model temperature.
- conv_round_limit: rounds limit for the conversation with context retrieval agent.
- selected_tasks_file: a file containing ids of all tasks to be run.
- enable_validation: inside the agent, whether to enable a validation loop which validates the candidate patches against the existing regression tests in the buggy version of the program.
- print: whether to the print more info to console.
- num_processes: number of parallel processes when running auto-code-rover. Should not be too large, otherwise parallelly running multiple task instances can exceed OpenAI/Anthropic token limit and cause the task instance to fail.
- notes: for book-keeping.

## Running the experiment

In your shell, start the experiment with:

```bash
conda activate auto-code-rover
export OPENAI_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxx # (or ANTHROPIC_API_KEY)
python scripts/run.py conf/example.conf
```

The run script runs AutoCodeRover on the task list defined in the conf file, and afterwards automatically runs SWE-bench evaluation to generate a report. Currently this script uses [SWE-bench-docker](https://github.com/aorwall/SWE-bench-docker) for the evaluation and report generation.

NOTE: For the final report on which tasks are resolved, please refer to `report/report.json` in the experiment output directory.

NOTE: All patches generated by AutoCodeRover is written to `predictions_for_swebench.json` file in the experiment output directory.
Instead of using SWE-bench-docker for evaluation, you can also separately set up the [containerized SWE-bench evaluation](https://github.com/princeton-nlp/SWE-bench/blob/main/docs/20240627_docker/README.md), and use that to evaluate the generated `predictions_for_swebench.json` file.

### Running experiments multiple times

When running multiple experiments (sequentially), we recommend using a different `id` in the conf
file for each experiment. This is because the outputs for an experiment will be created in a directory
using `id` as the name.

If you want to use the same `id` for multiple experiments and overwrite the previous experiment results,
you can use the `-f` option of `scripts/run.py`. This will remove the previous experiment results with
the same `id`.

### Note on running experiments in parallel

We do not recommend creating multiple processes running the script `scripts/run.py`. This is because
different tasks instances may share the same copy of local code base (e.g. `astropy-6938` and
`astropy-7746` share the same codebase at `setup_astropy__astropy__1.3`).

Instead, we support parallelism of experiments in `scripts/run.py` itself. Please set the value
of `num_processes` in the conf file to control how many tasks can be run in parallel. The scripts
internally handle the parallelism issue mentioned above.

## Contacts

> [!NOTE]
> If you encounter any issue in the tool or experiment, you can contact us via email at info@autocoderover.dev, or through our [discord server](https://discord.com/invite/ScXsdE49JY).
